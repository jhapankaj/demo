{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import gc\n",
    "import nltk\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_en = stopwords_en.union(string.punctuation).union({'\\(','\\)'}) - {'&','and','for'}\n",
    "import os\n",
    "\n",
    "from unidecode import unidecode\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(string):\n",
    "    \n",
    "    '''\n",
    "    Convert string to list of tokens and seperating non alaphabets from alpha numeric and extra spaces.\n",
    "    Args:\n",
    "        string: string to tokeinze\n",
    "    Returns:\n",
    "        List of tokens\n",
    "    '''\n",
    "    t = string.strip('\\n')[0]\n",
    "    for x in string.strip('\\n')[1:]:\n",
    "        if (str.isalpha(x) != str.isalpha(t[-1]) or str.isdigit(x) != str.isdigit(t[-1]) ) and x != ' ' :\n",
    "            if t[-1] != ' ':\n",
    "                t+= ' '\n",
    "            t += x\n",
    "            if not x.isalnum():\n",
    "                t += ' '\n",
    "        else: \n",
    "            t += x\n",
    "            if not x.isalnum():\n",
    "                t += ' '\n",
    "            \n",
    "    return t.strip()\n",
    "\n",
    "def get_stop_punc_split(sentences,stopwords_en,tokenize = True):\n",
    "    sentences_split = []\n",
    "    for sent in tqdm(sentences):\n",
    "        words = text_tokenizer(sent).strip().split() if tokenize else sent.strip().split()\n",
    "        tmp = []\n",
    "        for i in words:\n",
    "            if i in stopwords_en:\n",
    "                if ' '.join(tmp).strip('()') != '':\n",
    "                    sentences_split.append(' '.join(tmp).strip('()'))\n",
    "                tmp=[]\n",
    "            else:\n",
    "                if i.isdigit() and len(i) > 4:\n",
    "                    pass\n",
    "                else:\n",
    "                    tmp.append(i)\n",
    "        if len(tmp)>0 and ' '.join(tmp).strip('()') != '':\n",
    "            sentences_split.append(' '.join(tmp).strip('()'))\n",
    "    return sentences_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    out = []\n",
    "    for w in x.split():\n",
    "        if \"-\" in w:\n",
    "            if all([i.isnumeric() or len(i) < 3 for i in w.split(\"-\")]):\n",
    "                out.append(w)\n",
    "            elif all([i.isalpha() and len(i) > 1 for i in w.split(\"-\")]):\n",
    "                out += w.split(\"-\")\n",
    "            else:\n",
    "                out.append(w.replace(\"-\", \"\"))\n",
    "        elif \"/\" in w:\n",
    "            if all([i.isnumeric() for i in w.split(\"/\")]):\n",
    "                out.append(w)\n",
    "            elif all([i.isalpha() and len(i) > 1 for i in w.split(\"/\")]):\n",
    "                out += w.split(\"/\")\n",
    "            else:\n",
    "                out.append(w.replace(\"/\", \"\"))\n",
    "        elif \"&\" in w:\n",
    "            if w == \"&\":\n",
    "                out.append(w)\n",
    "            elif all([len(i) < 2 for i in w.split(\"&\")]):\n",
    "                out.append(w)\n",
    "            elif any([not i.isalpha() for i in w.split(\"&\")]):\n",
    "                out.append(w)\n",
    "            else:\n",
    "                out.append(\" & \".join(w.split(\"&\")))\n",
    "        elif \".\" in w:\n",
    "            if any([not i.isalpha() for i in w.split(\".\")]):\n",
    "                out.append(w)\n",
    "            elif all([i.isalpha() and len(i) > 1 for i in w.split(\".\")]):\n",
    "                out += w.split(\".\")\n",
    "            else:\n",
    "                out.append(w.replace(\".\", \"\"))\n",
    "        else:\n",
    "            out.append(w)\n",
    "    print(out)\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New  york\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80e66c5a3141a54f2b9afd771889fbd6e2de230c81c254fb1e98fd9647577081"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
